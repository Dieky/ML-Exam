{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# K-Means Clustering Analysis of Airbnb Data\n",
    "\n",
    "This notebook performs K-means clustering analysis on Airbnb listing data to identify distinct groups of listings with similar characteristics.\n",
    "\n",
    "## What is K-Means Clustering?\n",
    "\n",
    "K-means is an unsupervised machine learning algorithm that groups similar data points together (clustering) by trying to separate samples into n groups of equal variance, minimizing a criterion known as inertia or within-cluster sum-of-squares. \n",
    "\n",
    "The algorithm works by:\n",
    "1. Randomly selecting k initial centroids (k is specified by the user)\n",
    "2. Assigning each data point to the nearest centroid\n",
    "3. Recalculating the centroids as the mean of all points assigned to that cluster\n",
    "4. Repeating steps 2-3 until the centroids no longer change significantly\n",
    "\n",
    "For Airbnb data, clustering helps identify natural groupings in the marketplace - such as luxury properties, budget accommodations, or family-friendly options - which can provide insights for hosts, guests, and the platform itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "First, we import the necessary libraries for data manipulation, clustering, visualization and analysis. We use StandardScaler to normalize our features to have mean=0 and standard deviation=1, which ensures all features contribute equally to the clustering regardless of their original scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Load the cleaned Airbnb dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_airbnb_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Finding Optimal Number of Clusters\n",
    "\n",
    "One of the biggest challenges in K-means clustering is determining the optimal number of clusters (k). If k is too small, we might combine groups that should be separate; if k is too large, we might artificially split what should be one cohesive group.\n",
    "\n",
    "### The Elbow Method: How It Works and Why We Use It\n",
    "\n",
    "The elbow method is a visual and quantitative technique to find the appropriate number of clusters by plotting the inertia (within-cluster sum of squares) against different values of k. The process involves:\n",
    "\n",
    "1. **Data preparation**: Selecting and standardizing numerical features to ensure each feature contributes equally to distance calculations\n",
    "   - Standardization is crucial because features with larger scales (like price) would otherwise dominate the clustering compared to features with smaller scales (like ratings)\n",
    "\n",
    "2. **Running K-means with different k values**: We try a range of k values (1-12) to see how the model performs with different numbers of clusters\n",
    "\n",
    "3. **Calculating inertia for each k**: Inertia measures how internally coherent clusters are\n",
    "   - Mathematically, it's the sum of squared distances between each point and its assigned cluster center\n",
    "   - Lower inertia = points are closer to their centroids = more compact clusters\n",
    "   - Inertia will always decrease as k increases (with k=n, each point could be its own cluster with zero inertia)\n",
    "\n",
    "4. **Finding the 'elbow point'**: The point where adding another cluster gives diminishing returns\n",
    "   - This appears as a bend or \"elbow\" in the plot where the rate of inertia decrease slows significantly\n",
    "   - Intuitively, this is where we've captured most of the natural structure in the data\n",
    "\n",
    "### Using a Percentage Improvement Threshold\n",
    "\n",
    "Since eyeballing the elbow point can be subjective, we've added a simple percentage rule: when adding another cluster improves things by less than 8%, we stop there.\n",
    "\n",
    "Why 8%? We tried several values:\n",
    "- Higher values (10-15%) gave too few clusters, lumping together listings that seemed different\n",
    "- Lower values (3-5%) created too many clusters with minimal differences\n",
    "- 8% hit the sweet spot for our data, given the big jump between the 6th and 7th cluster.\n",
    "\n",
    "This gives us a clear stopping rule while still respecting the visual elbow method. You can adjust this threshold in the code if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Feature Selection for Clustering\n",
    "\n",
    "Choosing the right features for clustering is critical. We've selected features that capture different aspects of Airbnb listings:\n",
    "\n",
    "- **Price metrics**: `realSum` - the actual price paid by guests, one of the most important factors in accommodation choice\n",
    "- **Capacity and size**: `person_capacity`, `bedrooms` - indicate how many people can stay and the size of the property\n",
    "- **Quality indicators**: `cleanliness_rating`, `guest_satisfaction_overall` - measure guest experience and property quality\n",
    "- **Location metrics**: `dist` (distance to city center), `metro_dist` (distance to nearest metro/public transit) - location is a critical factor in accommodation value\n",
    "- **Host characteristics**: `host_is_superhost_bool` - whether the host has achieved Airbnb's Superhost status (0=no, 1=yes), indicating reliability and quality\n",
    "- **Booking attributes**: `Is_weekend_bool` - captures if the listing is mainly booked for weekends (0=no, 1=yes), suggesting different target travelers\n",
    "- **Property type**: `room_private_bool` - distinguishes between private rooms (1) and entire properties (0)\n",
    "\n",
    "Each of these features provides different information that helps the algorithm identify natural groupings in the data. The boolean features (ending with `_bool`) are encoded as 0/1 values, and K-means treats them as numerical distances just like other features. After standardization, these binary features will contribute meaningfully to the clustering based on their distribution in the dataset.\n",
    "\n",
    "### Why We Need Standardization\n",
    "\n",
    "K-means measures how similar listings are using distance calculations. Without standardization, our results would be skewed:\n",
    "- Prices $(20-500) would overwhelm everything else\n",
    "- Ratings (1-5 scale) would barely matter\n",
    "- Yes/no features (0-1) would be practically ignored\n",
    "\n",
    "Standardizing puts all features on equal footing, so a 1-star difference in ratings can be just as important as a $50 difference in price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for clustering\n",
    "numerical_features = ['realSum', 'person_capacity', 'cleanliness_rating', \n",
    "                      'guest_satisfaction_overall', 'bedrooms', 'dist', 'metro_dist',\n",
    "                      'host_is_superhost_bool','room_private_bool']\n",
    "\n",
    "# Create a copy of the data with just numerical features\n",
    "cluster_data = df[numerical_features].copy()\n",
    "\n",
    "# Standardize the data for clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cluster_data)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Find the elbow point using the rate of change\n",
    "inertia_array = np.array(inertia)\n",
    "# Calculate percentage of change in inertia\n",
    "inertia_diffs = np.diff(inertia_array) / inertia_array[:-1] * 100\n",
    "# Convert to absolute values (since changes are negative)\n",
    "inertia_diffs_abs = np.abs(inertia_diffs)\n",
    "\n",
    "# Find the last k value where improvement is above the threshold\n",
    "threshold = 8  # percentage improvement threshold\n",
    "optimal_k = 2  # default to 2 if no clear elbow\n",
    "\n",
    "# Identify the last k value where the improvement is above threshold\n",
    "last_above_threshold = None\n",
    "for i, diff_pct in enumerate(inertia_diffs_abs):\n",
    "    k_value = i + 2  # k value is i+2 (since we're looking at differences and k starts at 1)\n",
    "    if diff_pct >= threshold:\n",
    "        last_above_threshold = k_value\n",
    "\n",
    "# Set optimal_k to be the last k with improvement above threshold\n",
    "if last_above_threshold is not None:\n",
    "    optimal_k = last_above_threshold  # Use exactly the last k above threshold\n",
    "\n",
    "# Print the actual improvement percentages for clarity\n",
    "print(\"Improvement percentages for each additional cluster:\")\n",
    "for i, pct in enumerate(inertia_diffs_abs):\n",
    "    k_val = i + 2  # k starts at 2 for diffs\n",
    "    print(f\"  k={k_val}: {pct:.2f}% improvement\")\n",
    "    if k_val == last_above_threshold:\n",
    "        print(f\"  --> Last value above {threshold}% threshold\")\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Main plot - inertia\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(k_range, inertia, marker='o', linewidth=2)\n",
    "plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k = {optimal_k}')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Secondary plot - percentage improvement\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(range(2, len(k_range) + 1), inertia_diffs_abs, alpha=0.7)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold = {threshold}%')\n",
    "# Mark the optimal k\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', \n",
    "           label=f'Optimal k = {optimal_k}')\n",
    "plt.title('Percentage Improvement with Additional Cluster')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Improvement (%)')\n",
    "plt.xticks(range(2, len(k_range) + 1))\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Automatically determined optimal k = {optimal_k} (last cluster with improvement >= {threshold}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Performing K-Means Clustering\n",
    "\n",
    "Based on the elbow analysis and our percentage improvement threshold criterion, we apply the K-means algorithm with the optimal number of clusters determined above. The exact number will vary depending on the data, but our algorithm has selected the most appropriate value based on when additional clusters stop providing substantial benefit (> 8% improvement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Analyzing Cluster Distribution by City\n",
    "\n",
    "Cities have different Airbnb markets. Looking at how clusters spread across cities shows us:\n",
    "\n",
    "- Which cities have more luxury or budget properties\n",
    "- If some cities have similar property mixes\n",
    "- Where certain property types are missing\n",
    "\n",
    "This helps identify opportunities in different locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='City', hue='cluster')\n",
    "plt.title(\"Listings by City and Cluster\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Examining Cluster Characteristics\n",
    "\n",
    "After assigning each Airbnb listing to a cluster, we need to understand what makes each cluster distinct. A crucial step in cluster analysis is interpreting what each cluster represents in the real world.\n",
    "\n",
    "### Understanding Cluster Statistics\n",
    "\n",
    "We calculate summary statistics (means) for each feature within each cluster. This creates a \"profile\" for each cluster, helping us characterize what types of listings each cluster contains.\n",
    "\n",
    "For example, a cluster with high values for:\n",
    "- `realSum` (price) and `bedrooms` would suggest luxury or larger properties\n",
    "- `cleanliness_rating` and `guest_satisfaction_overall` might represent high-quality accommodations\n",
    "- Low `dist` and `metro_dist` would indicate centrally located properties\n",
    "\n",
    "### Making Fair Comparisons Between Features\n",
    "\n",
    "We need to compare apples to apples when looking at different features. Since prices range from $20-$500 while ratings are just 1-5, we use Z-scores to level the playing field:\n",
    "\n",
    "1. **Convert to relative scores**: We measure how far each value is from average (in standard deviations)\n",
    "\n",
    "2. **Scale everything to 0-1**: We then convert these scores to a simple 0-1 scale where 1 means \"highest in this feature\" and 0 means \"lowest\"\n",
    "\n",
    "This makes it easy to see at a glance what makes each cluster special:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary = df.groupby('cluster')[numerical_features].mean()\n",
    "cluster_summary\n",
    "df['cluster'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Detailed Cluster Analysis\n",
    "\n",
    "To gain deeper insights into what makes each cluster unique, we need a more intuitive way to visualize and compare cluster characteristics across multiple dimensions simultaneously.\n",
    "\n",
    "### Understanding Cluster Centers\n",
    "\n",
    "Each cluster is represented by its centroid - the average position of all points in that cluster across all features. These centroids define the \"prototype\" or \"typical example\" of each cluster. \n",
    "\n",
    "We transform these centroids back to their original scale (undoing the standardization) to make them interpretable in real-world terms (actual prices, ratings, distances, etc.).\n",
    "\n",
    "### Why Use a Radar Chart?\n",
    "\n",
    "While tables with numbers are precise, they can be difficult to interpret at a glance. Radar charts (also called spider or web charts) provide a powerful visual alternative:\n",
    "\n",
    "1. **Multi-dimensional visualization**: Each axis represents a different feature group\n",
    "2. **Intuitive shape comparison**: The shape formed by each cluster creates a visual \"fingerprint\" \n",
    "3. **Feature grouping**: We combine related features to simplify the visualization\n",
    "4. **Relative comparison**: The chart shows how clusters compare to each other on each dimension\n",
    "\n",
    "### How Feature Grouping Works\n",
    "\n",
    "To simplify the radar chart, we condense our 10 original features into 5 intuitive groups:\n",
    "\n",
    "- **Distance**: Average of `dist` (distance to city center) and `metro_dist` (distance to public transit) \n",
    "- **Accommodation**: Average of `bedrooms` and `person_capacity` to represent property size\n",
    "- **Quality**: Average of `cleanliness_rating` and `guest_satisfaction_overall` to represent overall experience\n",
    "- **Price**: Using `realSum` directly as the sole price metric\n",
    "- **Superhost**: Using `host_is_superhost_bool` directly\n",
    "\n",
    "For each cluster, we calculate the average value for each feature group. For example, the Distance value for a cluster is the average of the mean `dist` and mean `metro_dist` values for all listings in that cluster. This grouping makes patterns more apparent while preserving the most important dimensions of variation.\n",
    "\n",
    "### Interpreting the Radar Chart\n",
    "\n",
    "In our radar chart:\n",
    "- Each colored polygon represents one cluster\n",
    "- Each axis represents a feature group (Distance, Accommodation, Quality, Price, Superhost)\n",
    "- Higher values (closer to 1) mean that cluster scores highly on that feature group relative to other clusters\n",
    "- The distinctive shape of each polygon helps identify the cluster's unique characteristics \n",
    "\n",
    "For example, a cluster with high values on Price and Quality but low on Distance might represent \"Luxury properties further from city center\" - this pattern is instantly visible in the radar chart's shape.\n",
    "\n",
    "The combination of the detailed table and intuitive radar chart provides a comprehensive view of what makes each cluster distinct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters\n",
    "# Get the cluster centers and transform back to original scale\n",
    "centers = kmeans.cluster_centers_\n",
    "centers_original = scaler.inverse_transform(centers)\n",
    "\n",
    "# Create a DataFrame with the cluster centers\n",
    "centers_df = pd.DataFrame(centers_original, columns=numerical_features)\n",
    "centers_df.index = [f'Cluster {i}' for i in range(optimal_k)]\n",
    "\n",
    "# Display the cluster centers using a clean styled table with alternating row colors\n",
    "centers_df_styled = centers_df.round(2).style.set_caption(\"Cluster Centers (Original Scale)\")\\\n",
    "    .set_properties(**{\n",
    "        'text-align': 'center',\n",
    "        'color': 'black',\n",
    "    })\\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'caption', 'props': [('font-weight', 'bold'), ('font-size', '16px')]},\n",
    "        {'selector': 'th', 'props': [\n",
    "            ('text-align', 'center'), \n",
    "            ('background-color', '#d9d9d9'),\n",
    "            ('color', 'black'),\n",
    "            ('font-weight', 'bold')\n",
    "        ]},\n",
    "        {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f2f2f2')]},\n",
    "        {'selector': 'tbody tr:nth-child(odd)', 'props': [('background-color', 'white')]},\n",
    "    ])\n",
    "\n",
    "display(centers_df_styled)\n",
    "\n",
    "# Visualize cluster characteristics with a radar chart\n",
    "def radar_chart(df, title, colors=None):\n",
    "    # Define feature groups\n",
    "    feature_groups = {\n",
    "        'Distance': ['dist', 'metro_dist'],\n",
    "        'Accommodation': ['bedrooms', 'person_capacity'],\n",
    "        'Quality': ['cleanliness_rating', 'guest_satisfaction_overall'],\n",
    "        'Price': ['realSum'],\n",
    "        'Superhost': ['host_is_superhost_bool']\n",
    "    }\n",
    "    \n",
    "    # Create a new DataFrame to store group averages\n",
    "    group_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Calculate the average for each feature group\n",
    "    for group_name, features in feature_groups.items():\n",
    "        valid_features = [f for f in features if f in df.columns]\n",
    "        if valid_features:\n",
    "            group_df[group_name] = df[valid_features].mean(axis=1)\n",
    "    \n",
    "    categories = list(group_df.columns)\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Normalize the data for the radar chart\n",
    "    normalized_df = pd.DataFrame(index=group_df.index, columns=group_df.columns)\n",
    "    \n",
    "    for col in group_df.columns:\n",
    "        feature_values = group_df[col].values\n",
    "        normalized_values = stats.zscore(feature_values)\n",
    "        normalized_df[col] = normalized_values\n",
    "    \n",
    "    # Scale to [0,1] range\n",
    "    min_val = normalized_df.values.min()\n",
    "    max_val = normalized_df.values.max()\n",
    "    \n",
    "    for col in normalized_df.columns:\n",
    "        normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    print(\"Z-score normalized cluster means by feature group (scaled to [0,1] for visualization):\")\n",
    "    \n",
    "    # Style the normalized values table\n",
    "    norm_df_styled = normalized_df.round(2).style\\\n",
    "        .background_gradient(cmap='Blues')\\\n",
    "        .set_properties(**{\n",
    "            'text-align': 'center',\n",
    "            'font-weight': 'bold'\n",
    "        })\\\n",
    "        .set_table_styles([\n",
    "            {'selector': 'th', 'props': [\n",
    "                ('text-align', 'center'), \n",
    "                ('background-color', '#d9d9d9'),\n",
    "                ('color', 'black'),\n",
    "                ('font-weight', 'bold')\n",
    "            ]},\n",
    "        ])\n",
    "    \n",
    "    display(norm_df_styled)\n",
    "    \n",
    "    # Create the radar chart\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(group_df.index)))\n",
    "    \n",
    "    for i, cluster in enumerate(group_df.index):\n",
    "        values = normalized_df.loc[cluster, categories].values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, label=cluster, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.5\", \"0.75\"], color=\"grey\", size=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.title(title, size=20, y=1.1)\n",
    "    return fig, ax\n",
    "\n",
    "# Create the radar chart\n",
    "radar_chart(centers_df, \"Cluster Characteristics: Distance, Accommodation, Quality, Price, Superhost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Based on above characteristics for each cluster, these are the names we summarized them to\n",
    "\n",
    "Cluster 0: Large and Luxurious\n",
    "\n",
    "Cluster 1: No bang for your buck\n",
    "\n",
    "Cluster 2: Value Central\n",
    "\n",
    "Cluster 3: Small and Affordable\n",
    "\n",
    "Cluster 4: All-arounder\n",
    "\n",
    "Cluster 5: Distant Getaway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Visualizing Clusters in 2D\n",
    "\n",
    "Since we can't directly visualize data with 10 dimensions, we need to squash it down to 2D while keeping as much of the important patterns as possible.\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "Principal Component Analysis (PCA) is like finding the best camera angles to photograph a complex 3D object. It:\n",
    "\n",
    "1. Finds the angle showing the most variation (Principal Component 1)\n",
    "2. Then finds the next best angle (Principal Component 2) that shows what we missed\n",
    "\n",
    "It's like taking a photo of a crowd from above, then from the side - together they reveal the 3D shape.\n",
    "\n",
    "### Reading the Scatter Plot\n",
    "\n",
    "In our plot:\n",
    "- Each dot is an Airbnb listing, colored by its cluster\n",
    "- Bigger dots mean more expensive listings\n",
    "- Red X's mark cluster centers\n",
    "- Dots close together are similar listings\n",
    "- Well-separated groups suggest distinct listing types\n",
    "\n",
    "Usually PC1 (x-axis) captures price and size differences, while PC2 (y-axis) often shows quality and location patterns, but this varies by dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a dataframe with PCA results and cluster labels\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['cluster'] = df['cluster']\n",
    "pca_df['price'] = df['realSum']\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    hue='cluster',\n",
    "    size='price',\n",
    "    sizes=(20, 200),\n",
    "    palette='viridis',\n",
    "    data=pca_df,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add centers to the plot\n",
    "centers = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=1, marker='X', edgecolor='black')\n",
    "\n",
    "plt.title(f'K-Means Clustering of Airbnb Listings (PCA-Reduced, k={optimal_k})')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Save Clustered Data\n",
    "\n",
    "Save the data with cluster assignments for future use, removing any old cluster columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['cluster_4', 'cluster_6'], errors='ignore')  # `errors='ignore'` makes it safe if they don't exist\n",
    "df.to_csv(\"../data/clustered_airbnb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Reflection and Conclusions\n",
    "\n",
    "### Summary of Work\n",
    "\n",
    "In this project, we applied KMeans clustering to Airbnb data to uncover meaningful groupings among listings. Specifically, we:\n",
    "\n",
    "* Determined the optimal number of clusters using an improved elbow method\n",
    "* Identified distinct groups of similar properties based on shared features\n",
    "* Created visualizations to highlight what defines each cluster, such as price, location, or guest satisfaction\n",
    "* Explored the impact of normalization, where pre-normalization suggested 3â€“4 clusters, but post-normalization revealed six clearer groups due to balanced feature contributions\n",
    "* Examined how careful feature selection improved clustering, as removing less meaningful variables (such as isWeekend, multi\\_bool, room\\_shared\\_bool, biz\\_bool, room\\_type) helped reveal stronger underlying patterns\n",
    "\n",
    "### Practical Relevance\n",
    "\n",
    "These findings have important practical implications:\n",
    "\n",
    "For **hosts**:\n",
    "\n",
    "* Understand where a property fits within the local market landscape\n",
    "* Set more competitive prices by comparing to similar listings\n",
    "* Identify which improvements could move a listing into a higher-value cluster\n",
    "\n",
    "For **guests**:\n",
    "\n",
    "* Quickly identify listings that best match personal preferences\n",
    "* Better understand what to expect at different price levels\n",
    "* Recognize which features typically come bundled together\n",
    "\n",
    "For **Airbnb as a platform**:\n",
    "\n",
    "* Enhance property recommendation algorithms\n",
    "* Analyze differences in market dynamics across cities\n",
    "* Identify underserved segments or emerging market opportunities\n",
    "\n",
    "### Opportunities for Future Work\n",
    "\n",
    "Looking ahead, several extensions could deepen these insights:\n",
    "\n",
    "* Perform text analysis on property descriptions, reviews, or amenities to enrich cluster profiles\n",
    "* Track how clusters evolve over time, revealing shifts in the market\n",
    "* Map clusters geographically within each city to uncover spatial patterns\n",
    "* Explore sub-clustering within large groups to identify even more granular patterns\n",
    "\n",
    "The clustered dataset has been saved and is ready for these follow-up analyses, providing a strong foundation for both further research and real-world business applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
