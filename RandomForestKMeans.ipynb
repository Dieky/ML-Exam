{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-imports",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Setup\n",
    "\n",
    "This cell imports required libraries and loads the cleaned Airbnb dataset. We use:\n",
    "- pandas for data manipulation\n",
    "- scikit-learn for machine learning models and preprocessing\n",
    "- numpy for numerical operations\n",
    "- matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b26b5-35a3-44f1-a65f-56519b63a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../ML-Exam/data/cleaned_airbnb_data.csv\")\n",
    "\n",
    "# Show the first few rows and check column names\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf-model",
   "metadata": {},
   "source": [
    "## Initial Random Forest Model\n",
    "\n",
    "This cell builds and evaluates a Random Forest model to predict Airbnb prices:\n",
    "1. Encodes categorical features using one-hot encoding\n",
    "2. Splits data into training and test sets\n",
    "3. Trains a Random Forest model\n",
    "4. Evaluates model performance using MSE and R² metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ba239-7cfa-4a0d-b62e-f9f1ed9afd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=['room_type', 'City'], drop_first=True)\n",
    "\n",
    "# Step 2: Define features (X) and target (y)\n",
    "X = df_encoded.drop(columns=['realSum', 'ID'])\n",
    "y = df_encoded['realSum']\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5bf11-4571-42a5-b939-552877f94cb9",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "R² = 0.605 means the model explains about 60.5% of the variance in Airbnb prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "price-desc",
   "metadata": {},
   "source": [
    "## Price Distribution Analysis\n",
    "\n",
    "This cell examines the distribution of prices in the dataset to understand the range and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ac1be-f1f3-4550-9a0a-e10475e83099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['realSum'].describe(percentiles=[0.01, 0.25, 0.5, 0.75, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outlier-removal",
   "metadata": {},
   "source": [
    "## Improved Model with Outlier Removal\n",
    "\n",
    "This cell creates an improved model by:\n",
    "1. Removing price outliers above the 99th percentile\n",
    "2. Retraining the Random Forest model\n",
    "3. Evaluating the new model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258094-9386-42d8-a467-f6050825c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out listings above the 99th percentile\n",
    "threshold = df['realSum'].quantile(0.99)\n",
    "df_filtered = df[df['realSum'] < threshold]\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 10],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring='r2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit to training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "mse, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-data",
   "metadata": {},
   "source": [
    "## Save Filtered Dataset\n",
    "\n",
    "This cell saves the filtered dataset (without outliers) to a CSV file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a04cb1-5ee9-46a2-bcd6-b9a57f76b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_path = \"../ML-Exam/data/cleaned_airbnb_data.csv\"\n",
    "df_filtered.to_csv(filtered_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans-init",
   "metadata": {},
   "source": [
    "## Initial K-Means Clustering\n",
    "\n",
    "This cell performs initial K-means clustering:\n",
    "1. Prepares data by dropping non-numeric columns\n",
    "2. Standardizes features\n",
    "3. Applies K-means with 5 clusters\n",
    "4. Shows sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ad9fb-031e-4384-a6a0-a6483109019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric or ID columns for clustering\n",
    "df_cluster = df_filtered.drop(columns=['ID', 'room_type', 'City'])\n",
    "\n",
    "best_model, best_r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster-summary",
   "metadata": {},
   "source": [
    "## Cluster Summary Statistics\n",
    "\n",
    "This cell calculates and displays average feature values for each cluster to understand cluster characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258094-9386-42d8-a467-f6050825c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cluster)\n",
    "\n",
    "# Fit KMeans and assign labels\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "df_filtered['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Calculate and display average feature values per cluster\n",
    "cluster_summary = df_filtered.groupby('Cluster').mean(numeric_only=True)\n",
    "\n",
    "# Display to user\n",
    "cluster_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-viz",
   "metadata": {},
   "source": [
    "## Cluster Visualization - Heatmap\n",
    "\n",
    "This cell creates a heatmap visualization of cluster characteristics to better understand the patterns in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8338f-471e-44b8-8795-9632df4a869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert any remaining object columns to numeric\n",
    "for col in df_cluster.select_dtypes(include='object').columns:\n",
    "    df_cluster[col] = df_cluster[col].astype('category').cat.codes\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cluster)\n",
    "\n",
    "# Fit KMeans and assign cluster labels\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "df_filtered['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Compute average feature values per cluster\n",
    "cluster_summary = df_filtered.groupby('Cluster').mean(numeric_only=True)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cluster_summary, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Average Value')\n",
    "plt.xticks(ticks=range(len(cluster_summary.columns)), labels=cluster_summary.columns, rotation=90)\n",
    "plt.yticks(ticks=range(len(cluster_summary.index)), labels=[f'Cluster {i}' for i in cluster_summary.index])\n",
    "plt.title(\"Cluster Feature Averages (KMeans)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalized-heatmap",
   "metadata": {},
   "source": [
    "## Normalized Cluster Visualization\n",
    "\n",
    "This cell creates an improved heatmap with normalized values for better comparison between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5accf82-a55b-4483-8fe4-7f9c8459d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cluster)\n",
    "\n",
    "# Fit KMeans and assign cluster labels\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "df_filtered['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Recompute cluster averages without ID\n",
    "cluster_summary_clean = df_filtered.drop(columns=['ID']).groupby('Cluster').mean(numeric_only=True)\n",
    "\n",
    "# Normalize for heatmap clarity\n",
    "normalized_summary = (cluster_summary_clean - cluster_summary_clean.min()) / (cluster_summary_clean.max() - cluster_summary_clean.min())\n",
    "\n",
    "# Plot the improved heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(normalized_summary, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Normalized Average Value')\n",
    "plt.xticks(ticks=range(len(normalized_summary.columns)), labels=normalized_summary.columns, rotation=90)\n",
    "plt.yticks(ticks=range(len(normalized_summary.index)), labels=[f'Cluster {i}' for i in normalized_summary.index])\n",
    "plt.title(\"Normalized Cluster Feature Averages (KMeans)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elbow-method",
   "metadata": {},
   "source": [
    "## Optimal K Selection and Advanced Visualization\n",
    "\n",
    "This cell performs a comprehensive analysis to:\n",
    "1. Find the optimal number of clusters using the elbow method\n",
    "2. Create detailed visualizations of cluster characteristics\n",
    "3. Generate a radar chart for multi-dimensional feature comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing K-Means clustering on the dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Select numerical features for clustering\n",
    "numerical_features = ['realSum', 'person_capacity', 'cleanliness_rating', \n",
    "                      'guest_satisfaction_overall', 'bedrooms', 'dist', 'metro_dist',\n",
    "                      'host_is_superhost_bool']\n",
    "\n",
    "\n",
    "# Create a copy of the data with just numerical features\n",
    "cluster_data = df[numerical_features].copy()\n",
    "\n",
    "# Standardize the data for clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(cluster_data)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Find the elbow point using the rate of change\n",
    "inertia_array = np.array(inertia)\n",
    "# Calculate percentage of change in inertia\n",
    "inertia_diffs = np.diff(inertia_array) / inertia_array[:-1] * 100\n",
    "# Convert to absolute values (since changes are negative)\n",
    "inertia_diffs_abs = np.abs(inertia_diffs)\n",
    "\n",
    "# Find the last k value where improvement is above the threshold\n",
    "threshold = 10  # percentage improvement threshold\n",
    "optimal_k = 2  # default to 2 if no clear elbow\n",
    "\n",
    "# Identify the last k value where the improvement is above threshold\n",
    "last_above_threshold = None\n",
    "for i, diff_pct in enumerate(inertia_diffs_abs):\n",
    "    k_value = i + 2  # k value is i+2 (since we're looking at differences and k starts at 1)\n",
    "    if diff_pct >= threshold:\n",
    "        last_above_threshold = k_value\n",
    "\n",
    "# Set optimal_k to be the last k with improvement above threshold\n",
    "if last_above_threshold is not None:\n",
    "    optimal_k = last_above_threshold  # Use exactly the last k above threshold\n",
    "\n",
    "# Print the actual improvement percentages for clarity\n",
    "print(\"Improvement percentages for each additional cluster:\")\n",
    "for i, pct in enumerate(inertia_diffs_abs):\n",
    "    k_val = i + 2  # k starts at 2 for diffs\n",
    "    print(f\"  k={k_val}: {pct:.2f}% improvement\")\n",
    "    if k_val == last_above_threshold:\n",
    "        print(f\"  --> Last value above {threshold}% threshold\")\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Main plot - inertia\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(k_range, inertia, marker='o', linewidth=2)\n",
    "plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k = {optimal_k}')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Secondary plot - percentage improvement\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(range(2, len(k_range) + 1), inertia_diffs_abs, alpha=0.7)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold = {threshold}%')\n",
    "# Mark the optimal k\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', \n",
    "           label=f'Optimal k = {optimal_k}')\n",
    "plt.title('Percentage Improvement with Additional Cluster')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Improvement (%)')\n",
    "plt.xticks(range(2, len(k_range) + 1))\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Automatically determined optimal k = {optimal_k} (last cluster with improvement >= {threshold}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster-viz",
   "metadata": {},
   "source": [
    "## Final Cluster Visualization and Analysis\n",
    "\n",
    "This cell creates the final visualization of clusters using:\n",
    "1. PCA for dimensionality reduction\n",
    "2. Scatter plot with cluster centers\n",
    "3. Price-based point sizing for additional insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb81d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with the optimal K determined from the elbow curve\n",
    "# No need to hardcode optimal_k - it was calculated in the previous cell\n",
    "\n",
    "# Apply K-Means with the optimal K\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Apply PCA to reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a dataframe with PCA results and cluster labels\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['cluster'] = cluster_labels\n",
    "pca_df['price'] = df['realSum']\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    hue='cluster',\n",
    "    size='price',\n",
    "    sizes=(20, 200),\n",
    "    palette='viridis',\n",
    "    data=pca_df,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add centers to the plot\n",
    "centers = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=1, marker='X', edgecolor='black')\n",
    "\n",
    "plt.title(f'K-Means Clustering of Airbnb Listings (PCA-Reduced, k={optimal_k})')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster-analysis",
   "metadata": {},
   "source": [
    "## Detailed Cluster Analysis\n",
    "\n",
    "This cell performs a comprehensive analysis of cluster characteristics using:\n",
    "1. Feature group analysis\n",
    "2. Radar chart visualization\n",
    "3. Z-score normalization for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8cef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters\n",
    "# Get the cluster centers and transform back to original scale\n",
    "centers = kmeans.cluster_centers_\n",
    "centers_original = scaler.inverse_transform(centers)\n",
    "\n",
    "# Create a DataFrame with the cluster centers\n",
    "centers_df = pd.DataFrame(centers_original, columns=numerical_features)\n",
    "centers_df.index = [f'Cluster {i}' for i in range(optimal_k)]\n",
    "\n",
    "# Display the cluster centers using a clean styled table with alternating row colors\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Format cluster centers table with alternating rows\n",
    "centers_df_styled = centers_df.round(2).style.set_caption(\"Cluster Centers (Original Scale)\")\\\n",
    "    .set_properties(**{\n",
    "        'text-align': 'center',\n",
    "        'color': 'black',\n",
    "    })\\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'caption', 'props': [('font-weight', 'bold'), ('font-size', '16px')]},\n",
    "        {'selector': 'th', 'props': [\n",
    "            ('text-align', 'center'), \n",
    "            ('background-color', '#d9d9d9'),\n",
    "            ('color', 'black'),\n",
    "            ('font-weight', 'bold')\n",
    "        ]},\n",
    "        # Add alternating row colors\n",
    "        {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f2f2f2')]},\n",
    "        {'selector': 'tbody tr:nth-child(odd)', 'props': [('background-color', 'white')]},\n",
    "    ])\n",
    "\n",
    "display(centers_df_styled)\n",
    "\n",
    "# Count listings per cluster\n",
    "cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "cluster_counts.index = [f'Cluster {i}' for i in range(optimal_k)]\n",
    "\n",
    "# Create a DataFrame for the counts with percentages\n",
    "count_df = pd.DataFrame({\n",
    "    'Count': cluster_counts,\n",
    "    'Percentage': (cluster_counts / cluster_counts.sum() * 100).round(1).astype(str) + '%'\n",
    "})\n",
    "\n",
    "# Display cluster counts in a styled table\n",
    "count_df_styled = count_df.style.set_caption(\"Listings per Cluster\")\\\n",
    "    .set_properties(**{\n",
    "        'text-align': 'center',\n",
    "        'color': 'black',\n",
    "    })\\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'caption', 'props': [('font-weight', 'bold'), ('font-size', '16px')]},\n",
    "        {'selector': 'th', 'props': [\n",
    "            ('text-align', 'center'), \n",
    "            ('background-color', '#d9d9d9'),\n",
    "            ('color', 'black'),\n",
    "            ('font-weight', 'bold')\n",
    "        ]},\n",
    "        {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f2f2f2')]},\n",
    "        {'selector': 'tbody tr:nth-child(odd)', 'props': [('background-color', 'white')]},\n",
    "    ])\n",
    "\n",
    "print(\"\\nCluster Sizes:\")\n",
    "display(count_df_styled)\n",
    "\n",
    "# Visualize cluster characteristics with a radar chart\n",
    "def radar_chart(df, title, colors=None):\n",
    "    # Define feature groups - only include the requested ones\n",
    "    feature_groups = {\n",
    "        'Location': ['dist', 'metro_dist'],\n",
    "        'Accommodation': ['bedrooms', 'person_capacity'],\n",
    "        'Quality': ['cleanliness_rating', 'guest_satisfaction_overall'],\n",
    "        'Price': ['realSum'],\n",
    "        'Superhost': ['host_is_superhost_bool']\n",
    "    }\n",
    "    \n",
    "    # Create a new DataFrame to store group averages\n",
    "    group_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Calculate the average for each feature group\n",
    "    for group_name, features in feature_groups.items():\n",
    "        # Filter to only include features that exist in our dataset\n",
    "        valid_features = [f for f in features if f in df.columns]\n",
    "        if valid_features:\n",
    "            # Calculate the mean of all features in this group for each cluster\n",
    "            group_df[group_name] = df[valid_features].mean(axis=1)\n",
    "    \n",
    "    # Number of variables (now feature groups instead of individual features)\n",
    "    categories = list(group_df.columns)\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Normalize the data for the radar chart using Z-score normalization\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Create a copy of the dataframe for normalization\n",
    "    normalized_df = pd.DataFrame(index=group_df.index, columns=group_df.columns)\n",
    "    \n",
    "    # Scale each feature group independently using Z-score normalization\n",
    "    for col in group_df.columns:\n",
    "        # Extract the column values\n",
    "        feature_values = group_df[col].values\n",
    "        # Apply Z-score normalization (mean=0, std=1)\n",
    "        normalized_values = stats.zscore(feature_values)\n",
    "        # Store in the normalized dataframe\n",
    "        normalized_df[col] = normalized_values\n",
    "    \n",
    "    # Scale the Z-scores to [0,1] range for radar chart visualization\n",
    "    # First find the min and max across all features\n",
    "    min_val = normalized_df.values.min()\n",
    "    max_val = normalized_df.values.max()\n",
    "    \n",
    "    # Scale to [0,1] range\n",
    "    for col in normalized_df.columns:\n",
    "        normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    print(\"Z-score normalized cluster means by feature group (scaled to [0,1] for visualization):\")\n",
    "    \n",
    "    # Style the normalized values table with heat map coloring\n",
    "    norm_df_styled = normalized_df.round(2).style\\\n",
    "        .background_gradient(cmap='Blues')\\\n",
    "        .set_properties(**{\n",
    "            'text-align': 'center',\n",
    "            'font-weight': 'bold'\n",
    "        })\\\n",
    "        .set_table_styles([\n",
    "            {'selector': 'th', 'props': [\n",
    "                ('text-align', 'center'), \n",
    "                ('background-color', '#d9d9d9'),\n",
    "                ('color', 'black'),\n",
    "                ('font-weight', 'bold')\n",
    "            ]},\n",
    "        ])\n",
    "    \n",
    "    display(norm_df_styled)\n",
    "    \n",
    "    # Calculate angles for the radar chart\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # If no colors provided, use default colormap\n",
    "    if colors is None:\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(group_df.index)))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster in enumerate(group_df.index):\n",
    "        values = normalized_df.loc[cluster, categories].values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, label=cluster, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    # Set category labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12)\n",
    "    \n",
    "    # Add gridlines and adjust their appearance\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.5\", \"0.75\"], color=\"grey\", size=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.title(title, size=20, y=1.1)\n",
    "    return fig, ax\n",
    "\n",
    "# Create the radar chart with Z-score normalization based on the requested feature groups\n",
    "radar_chart(centers_df, \"Cluster Characteristics: Location, Accommodation, Quality, Price, Superhost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7fe54",
   "metadata": {},
   "source": [
    "## K-Means Clustering Analysis\n",
    "\n",
    "The K-Means clustering helps us identify natural groupings in the Airbnb listings data. The process was:\n",
    "\n",
    "1. **Feature Selection**: Used numerical features like price, capacity, ratings, etc.\n",
    "2. **Data Standardization**: Scaled features to have mean=0 and std=1\n",
    "3. **Optimal K Selection**: Used the elbow method to determine the best number of clusters\n",
    "4. **Dimensionality Reduction**: Applied PCA to visualize high-dimensional data in 2D\n",
    "5. **Visualization**: Created scatter plot of clusters and radar chart of cluster characteristics\n",
    "\n",
    "### Interpretation of Results:\n",
    "\n",
    "- **Cluster Visualization**: The scatter plot shows the distribution of listings in 2D space, with colors representing different clusters and size indicating price\n",
    "- **Cluster Centers**: The table shows the average values of each feature for each cluster\n",
    "- **Radar Chart**: Displays the relative values of features across clusters, helping identify what makes each cluster unique\n",
    "- **Feature Normalization**: Each feature is independently normalized across clusters to better highlight differences\n",
    "\n",
    "This clustering can help hosts and travelers understand different market segments within the Airbnb ecosystem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
