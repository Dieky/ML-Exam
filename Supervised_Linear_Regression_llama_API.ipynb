{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b7d438-37e2-4bd9-ad07-186a05565847",
   "metadata": {},
   "source": [
    "# Predicting a price with llama \n",
    "In the first part of this project we used machine learning to suggest a price for a new listing based on linear regression.\n",
    "\n",
    "This notebook attempts to do the same but now we let AI suggest a price instead. The model we are using is llama3. There will be 2 different approaches to using the model.\n",
    "\n",
    "- No data set\n",
    "- Feeding a data set through RAG (Retrieval-Augmented Generation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0523c-8f63-4c8d-b5e2-67ffd0b846b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b44a9b-eddb-4c91-b8f8-eae15e648306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52fd40d-49a1-44d7-bced-1fbae2dd11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/supervised_cleaned_airbnb_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd82f32-62ab-4d9c-983e-e015e09a9f06",
   "metadata": {},
   "source": [
    "## No data set\n",
    "\n",
    "We wanted to see how well the model could handle the task without feeding it any context, such as the data set. The query is sent to the llama API and we got some varying responses. The suggested price was usually in the range 120-170$ with some big outliers every so often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca1606-c081-4ad2-8a4d-539a0d65e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straight query without feeding the model data through RAG\n",
    "\n",
    "query = \"\"\"\n",
    "A 1-bedroom apartment in Amsterdam with room for 2 people, a max distance to the city center 5km and metro distance of 2,5km. Outside of weekends\n",
    "Estimate a fair nightly price in USD for this property as if it were listed on Airbnb.\n",
    "Only return the price as a number.\n",
    "\"\"\"\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": query,\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "price = response.json()[\"response\"]\n",
    "print(f\"Ollama estimated price: {price}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2db93-c6a1-4f7c-9d0b-deda8c16d8c9",
   "metadata": {},
   "source": [
    "## Preparing RAG\n",
    "\n",
    "We can not feed the raw csv file to llama through the API. So we have to convert the file into a string that we can send through the API. Each row is converted (descriptions) then the embedder model takes each converted string and converts the string to a vector, which is then added to a numpy array.\n",
    "\n",
    "The numpy array is then used to create FAISS indexes which helps find similar listings in a very efficient way (comparing the Euclidean distance of vectors) The dataset is now ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f6ae9c-d5e8-4aca-b7e4-61513090b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done!\n"
     ]
    }
   ],
   "source": [
    "# Prepare data using RAG\n",
    "\n",
    "# returns a string format of the row and the features we wish to use\n",
    "def row_to_description(row):\n",
    "    return (\n",
    "        f\"A {row['bedrooms']}-bedroom apartment in {row['City']} with room for {row['person_capacity']} people, \"\n",
    "        f\"max distance to the city center is {row['dist']} km and metro distance is {row['metro_dist']} km. \"\n",
    "        f\"Nightly price: ${row['realSum']}\"\n",
    "    )\n",
    "\n",
    "# Generate descriptions for all listings\n",
    "descriptions = df.apply(row_to_description, axis=1).tolist()\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2') # this model gives a vector 384 values\n",
    "\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedder.encode(descriptions, convert_to_numpy=True)\n",
    "\n",
    "# example: embedder.encode(\"Cozy apartment in Amsterdam\")\n",
    "# Could return a vector -> [0.01, -0.12, 0.38, ..., -0.02]  with a total of 384 values\n",
    "# These vectors can then be compared for similarities\n",
    "\n",
    "# Create FAISS index\n",
    "# Creates a searchable vector \"database\" using simple but fast math.\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension) \n",
    "index.add(embeddings)\n",
    "# The index is now ready to answer questions like:\n",
    "# \"What are the 10 most similar listings to this new description?\"\n",
    "\n",
    "print(\"Embedding done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e7b0c-1051-45d7-9f9a-1f3a610581d5",
   "metadata": {},
   "source": [
    "## Creating queries\n",
    "\n",
    "Creating a good query is necessary when working with AI. We tested some different approaches and had varying results.\n",
    "- First prompt with no extra descriptions or 4T's. Just the actual data we want to look for.\n",
    "- Using 4T's to create a more descriptive query.\n",
    "- Modified version of first prompt.\n",
    "- Copy paste the row_to_description method return value.\n",
    "\n",
    "We also tested with a top_k of 5 and 10 to see if the results were better depending on this variable.\n",
    "\n",
    "For all queries we went for the same values and our intended price target would be $194.03 (matching the first entry in the data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a753525-5709-4fc9-8942-ec6e3eee6c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama (RAG-based) estimated price: Based on the provided listings, I estimate the price for this listing to be:\n",
      "\n",
      "$342.11\n"
     ]
    }
   ],
   "source": [
    "# Queries and results\n",
    "# \"query\" is the active variable used for the model. Rename variables to test out different queries.\n",
    "# We are using the first entry in our data set as a description. The realSum is 194.03.\n",
    "\n",
    "# Original prompt with no focus on the 4'ts.\n",
    "# Result = $137.5 || top_k = 10\n",
    "# Result = $108.1 || top_k = 5\n",
    "query1 = \"\"\"\n",
    "A 1-bedroom apartment in City 1 with room for 2 people, a distance to the city center 5km and metro distance of 2.5km. Outside of weekends\n",
    "\"\"\"\n",
    "\n",
    "# Using the 4 T's in our prompt.\n",
    "# Result = $123 || top_k = 10\n",
    "# Result = $304.65 || top_k = 5\n",
    "query2 = \"\"\"\n",
    "Calculate a recommended price for a listing with these parameters.\n",
    "\n",
    "A 1-bedroom apartment in Amsterdam with room for 2 people, a distance to the city center 5km and metro distance of 2.5km. Outside of weekends\n",
    "\n",
    "The city values in the data set are transformed to numeric values. These are the original values: 1. Amsterdam, 2. Athen, 3. Barcelona, 4. Berlin, 5. Budapest, 6. Lisbon, 7. London, 8. Paris, 9. Rome, 10. Vienna\n",
    "\n",
    "Only return the price as a number.\n",
    "\"\"\"\n",
    "\n",
    "# Original prompt with City changed to Amsterdam instead of city 1 and less focus on the 4 T's.\n",
    "# Result = $112.45 || top_k = 10\n",
    "# Result = $122.25 || top_k = 5\n",
    "query3 = \"\"\"\n",
    "A 1-bedroom apartment in Amsterdam with room for 2 people, a distance to the city center 5km and metro distance of 2.5km. Outside of weekends.\n",
    "\n",
    "The city values in the data set are transformed to numeric values. These are the original values: 1. Amsterdam, 2. Athen, 3. Barcelona, 4. Berlin, 5. Budapest, 6. Lisbon, 7. London, 8. Paris, 9. Rome, 10. Vienna\n",
    "\"\"\"\n",
    "\n",
    "# Prompt created by copy pasting the row_to_description method string output with the values of the first row in the data set\n",
    "# Result = $394.39 || top_k = 10\n",
    "# Result = $264.1 || top_k = 5\n",
    "query4 = \"\"\"\n",
    "A 1-bedroom apartment in 1 with room for 2 people,\n",
    "max distance to the city center is 5 km and metro distance is 2.5 km.\n",
    "\"\"\"\n",
    "\n",
    "# Creates a vector of our query that can be used to look for similar euclidean distances\n",
    "query_embedding = embedder.encode([query])\n",
    "\n",
    "# Search top 5/10\n",
    "top_k = 5\n",
    "distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "# Retrieves the original text before vectorizing the string.\n",
    "# example from earlier now goes from vector to string.\n",
    "# [0.01, -0.12, 0.38, ..., -0.02] -> \"Cozy apartment in Amsterdam\"\n",
    "retrieved_examples = [descriptions[i] for i in indices[0]]\n",
    "\n",
    "\n",
    "# Combine the retrieved examples to a string that we can pass to the API request\n",
    "context = \"\\n\".join([f\"{i+1}. {desc}\" for i, desc in enumerate(retrieved_examples)])\n",
    "\n",
    "final_prompt = f\"\"\"\n",
    "Here are some similar Airbnb listings and their prices:\n",
    "\n",
    "{context}\n",
    "\n",
    "Now estimate the price for this listing:\n",
    "{query}\n",
    "\n",
    "Only return the estimated nightly price in USD as a number.\n",
    "\"\"\"\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": final_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "price = response.json()[\"response\"]\n",
    "print(f\"Ollama (RAG-based) estimated price: {price}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d626ae-bbba-423c-b074-56e25f98e869",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We expected that our first prompt would yield poor results and the following queries would improve the result. However this was not the case. Adding the 4T's gave some of the worst results. This was very surprising to us. \n",
    "\n",
    "As our data set contained City as a numerical value, we tried with the actual city name instead and gave the query an explanation of the values. Our hope was that this could give a good result and a query could have an actual city name instead of a numerical value, making it more suited for humans to use.\n",
    "\n",
    "As we dug deeper into understanding the selection process of top_k and what the vector comparison did, we tried copy pasting what the first row in the dataset would look like after the row_to_description. This should minimize the Euclidean distance in the vector and our expectation was to get a very good result. This was not the case, as this was actually the worst result we got.\n",
    "\n",
    "When we compare the results of using AI to estimate a price and comparing it with our machine learning model, the model wins over the AI. No matter how much we tweaked our queries we could not improve the results by feeding the AI some data from our data set. The response we got from sending just a query and letting the AI do its thing were simply just better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6d21c-1732-4611-950b-12c3e8f14ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
